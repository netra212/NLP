{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d687efb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-1.8.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting peft\n",
      "  Using cached peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting numpy<3.0.0,>=1.17 (from accelerate)\n",
      "  Using cached numpy-2.3.0-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.13/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.13/site-packages (from accelerate) (7.0.0)\n",
      "Collecting pyyaml (from accelerate)\n",
      "  Using cached PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting torch>=2.0.0 (from accelerate)\n",
      "  Downloading torch-2.7.1-cp313-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Collecting huggingface_hub>=0.21.0 (from accelerate)\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting safetensors>=0.4.3 (from accelerate)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub>=0.21.0->accelerate)\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub>=0.21.0->accelerate)\n",
      "  Downloading hf_xet-1.1.4-cp37-abi3-macosx_11_0_arm64.whl.metadata (879 bytes)\n",
      "Collecting scipy (from bitsandbytes)\n",
      "  Using cached scipy-1.15.3-cp313-cp313-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting setuptools (from torch>=2.0.0->accelerate)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=2.0.0->accelerate)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=2.0.0->accelerate)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=2.0.0->accelerate)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=2.0.0->accelerate)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=2.0.0->accelerate)\n",
      "  Using cached MarkupSafe-3.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.13/site-packages (from requests->transformers) (2025.6.15)\n",
      "Downloading accelerate-1.8.0-py3-none-any.whl (365 kB)\n",
      "Using cached numpy-2.3.0-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Downloading hf_xet-1.1.4-cp37-abi3-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Using cached PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl (171 kB)\n",
      "Downloading regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Downloading torch-2.7.1-cp313-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp313-cp313-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl (199 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached scipy-1.15.3-cp313-cp313-macosx_14_0_arm64.whl (22.4 MB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Installing collected packages: mpmath, urllib3, sympy, setuptools, safetensors, regex, pyyaml, numpy, networkx, MarkupSafe, hf-xet, fsspec, filelock, charset_normalizer, scipy, requests, jinja2, torch, huggingface_hub, bitsandbytes, tokenizers, accelerate, transformers, peft\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24/24\u001b[0m [peft]2m23/24\u001b[0m [peft]formers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 accelerate-1.8.0 bitsandbytes-0.42.0 charset_normalizer-3.4.2 filelock-3.18.0 fsspec-2025.5.1 hf-xet-1.1.4 huggingface_hub-0.33.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-2.3.0 peft-0.15.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.4 safetensors-0.5.3 scipy-1.15.3 setuptools-80.9.0 sympy-1.14.0 tokenizers-0.21.1 torch-2.7.1 transformers-4.52.4 urllib3-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate transformers peft bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb1e717c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.13/site-packages (from datasets) (2.3.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-20.0.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.3.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./venv/lib/python3.13/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./venv/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./venv/lib/python3.13/site-packages (from datasets) (0.33.0)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.13/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.13/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohttp-3.12.13-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.6 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached frozenlist-1.7.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.5.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached propcache-0.3.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached yarl-1.20.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: idna>=2.0 in ./venv/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./venv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached aiohttp-3.12.13-cp313-cp313-macosx_11_0_arm64.whl (464 kB)\n",
      "Downloading multidict-6.5.0-cp313-cp313-macosx_11_0_arm64.whl (42 kB)\n",
      "Using cached yarl-1.20.1-cp313-cp313-macosx_11_0_arm64.whl (88 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached frozenlist-1.7.0-cp313-cp313-macosx_11_0_arm64.whl (45 kB)\n",
      "Using cached propcache-0.3.2-cp313-cp313-macosx_11_0_arm64.whl (41 kB)\n",
      "Using cached pyarrow-20.0.0-cp313-cp313-macosx_12_0_arm64.whl (30.8 MB)\n",
      "Using cached pandas-2.3.0-cp313-cp313-macosx_11_0_arm64.whl (10.7 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading xxhash-3.5.0-cp313-cp313-macosx_11_0_arm64.whl (30 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, pyarrow, propcache, multidict, fsspec, frozenlist, dill, attrs, aiohappyeyeballs, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
      "\u001b[2K  Attempting uninstall: fsspec[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/17\u001b[0m [pyarrow]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.5.1━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/17\u001b[0m [pyarrow]\n",
      "\u001b[2K    Uninstalling fsspec-2025.5.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/17\u001b[0m [pyarrow]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.5.1━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/17\u001b[0m [pyarrow]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/17\u001b[0m [datasets]/17\u001b[0m [datasets]ess]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.13 aiosignal-1.3.2 attrs-25.3.0 datasets-3.6.0 dill-0.3.8 frozenlist-1.7.0 fsspec-2025.3.0 multidict-6.5.0 multiprocess-0.70.16 pandas-2.3.0 propcache-0.3.2 pyarrow-20.0.0 pytz-2025.2 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23415146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig, DataCollatorForLanguageModeling)\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ba790b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in ./venv/lib/python3.13/site-packages (0.42.0)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.13/site-packages (from bitsandbytes) (1.15.3)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in ./venv/lib/python3.13/site-packages (from scipy->bitsandbytes) (2.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "520ea8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./venv/lib/python3.13/site-packages (2.7.1)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.7.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.13/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.13/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.13/site-packages (from torchvision) (2.3.0)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Using cached pillow-11.2.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torchvision-0.22.1-cp313-cp313-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.7.1-cp313-cp313-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pillow-11.2.1-cp313-cp313-macosx_11_0_arm64.whl (3.0 MB)\n",
      "Installing collected packages: pillow, torchvision, torchaudio\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [torchaudio]3\u001b[0m [torchaudio]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed pillow-11.2.1 torchaudio-2.7.1 torchvision-0.22.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fc11343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple GPU via MPS\n"
     ]
    }
   ],
   "source": [
    "# Check for MPS (Metal Performance Shaders) GPU support\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple GPU via MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "628d25e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization of the class. \n",
      "starting fine-tunning process. \n",
      "Loaded Tokenizer.\n",
      "Tokenizer Loaded.\n",
      "Loaded Model.\n",
      "Model Loaded\n",
      "Implementing Lora. \n",
      "trainable params: 3,145,728 || all params: 1,421,416,448 || trainable%: 0.2213\n",
      "Implementing LoRA.\n",
      "Load data and tokenization. \n",
      "question\n",
      "0       question: Natalia sold clips to 48 of her frie...\n",
      "1       question: Weng earns $12 an hour for babysitti...\n",
      "2       question: Betty is saving money for a new wall...\n",
      "3       question: Julie is reading a 120-page book. Ye...\n",
      "4       question: James writes a 3-page letter to 2 di...\n",
      "                              ...                        \n",
      "7468    question: Very early this morning, Elise left ...\n",
      "7469    question: Josh is saving up for a box of cooki...\n",
      "7470    question: Colin can skip at six times the spee...\n",
      "7471    question: Janet, a third grade teacher, is pic...\n",
      "7472    question: At 30, Anika is 4/3 the age of Maddi...\n",
      "Name: text, Length: 7473, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing data: 100%|██████████| 7473/7473 [00:00<00:00, 8693.80 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded and tokenized.\n",
      "Running train. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 168\u001b[39m\n\u001b[32m    164\u001b[39m output_dir = \u001b[33m\"\u001b[39m\u001b[33mphi-1_5-finetuned\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    166\u001b[39m fine_tunner = LoRAFineTuner(model_name,dataset_name,output_dir)\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m \u001b[43mfine_tunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 155\u001b[39m, in \u001b[36mLoRAFineTuner.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28mself\u001b[39m.load_and_tokenize_dataset()\n\u001b[32m    153\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDataset loaded and tokenized.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTrain the model.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    158\u001b[39m \u001b[38;5;28mself\u001b[39m.merge_and_save_model()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mLoRAFineTuner.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"This function will perform the training.\"\"\"\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning train. \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     98\u001b[39m training_args = TrainingArguments(\n\u001b[32m     99\u001b[39m     output_dir = \u001b[38;5;28mself\u001b[39m.output_dir, \n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     per_device_train_batch_size = \u001b[43mbatch_size\u001b[49m, \n\u001b[32m    101\u001b[39m     gradient_accumulation_steps = \u001b[32m1\u001b[39m, \n\u001b[32m    102\u001b[39m     learning_rate = learning_rate, \n\u001b[32m    103\u001b[39m     lr_scheduler_type = \u001b[33m\"\u001b[39m\u001b[33mcosine\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    104\u001b[39m     save_strategy = \u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    105\u001b[39m     logging_steps = \u001b[32m100\u001b[39m, \n\u001b[32m    106\u001b[39m     max_steps = max_steps, \n\u001b[32m    107\u001b[39m     num_train_epochs = epochs, \n\u001b[32m    108\u001b[39m     push_to_hub = \u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[32m    109\u001b[39m     report_to = \u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    110\u001b[39m )\n\u001b[32m    112\u001b[39m trainer = Trainer(\n\u001b[32m    113\u001b[39m     model = \u001b[38;5;28mself\u001b[39m.model, \n\u001b[32m    114\u001b[39m     train_dataset = \u001b[38;5;28mself\u001b[39m.tokenized_data, \n\u001b[32m    115\u001b[39m     args = training_args, \n\u001b[32m    116\u001b[39m     data_collator = DataCollatorForLanguageModeling(\u001b[38;5;28mself\u001b[39m.tokenizer, mlm=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    117\u001b[39m )\n\u001b[32m    119\u001b[39m trainer.train()\n",
      "\u001b[31mNameError\u001b[39m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "class LoRAFineTuner:\n",
    "    def __init__(self, model_name,dataset_name,output_dir):\n",
    "        \"\"\"\n",
    "        This is initialization of the class parameter. \n",
    "        \"\"\"\n",
    "        print(\"Initialization of the class. \")\n",
    "        self.model_name = model_name\n",
    "        self.dataset_name = dataset_name\n",
    "        self.output_dir = output_dir\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.tokenized_data = None\n",
    "\n",
    "    def load_tokenizer(self):\n",
    "        \"\"\"This function to define the tokenizer of the model.\"\"\"\n",
    "        print(\"Loaded Tokenizer.\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"This function to define the model.\"\"\"\n",
    "        print(\"Loaded Model.\")\n",
    "\n",
    "        # Bits and Bytes Configs for the Quantized Models. \n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit = True, # Load the model weights in 4-bit precision (instead of 16/32-bit)\n",
    "            bnb_4bit_use_double_quant = True, # Enabling the double quantization which helps in preserving the accuracy. \n",
    "            bnb_4bit_quant_type = \"nf4\", # normal float 4, a quantized type optimized for transformers.  \n",
    "            bnb_4bit_compute_dtype = torch.float16 # Reducing the precisions of the model. \n",
    "        )\n",
    "        \n",
    "        # Quantization model.\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name, \n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code = True # allowing custom model to use. \n",
    "            # quantization_config = bnb_config # Applies the 4-bit quantization setup from earlier\n",
    "        )\n",
    "\n",
    "        self.model.config.use_cache = False # During training, using cache = True, may causes issues with backpropagation in casual langauge model. Disabling it ensures the model doesn’t store outputs for reuse (which is good for inference, but bad for training).\n",
    "\n",
    "    def apply_lora(self):\n",
    "        \"\"\"this function to define the lora model of the model. \"\"\"\n",
    "        print(\"Implementing Lora. \")\n",
    "        # LoRA - Low rank adapter to save compute and memory while training.\n",
    "        config = LoraConfig(\n",
    "            r=16,  # size of the low-rank matrices, smaller is better to save the memory. \n",
    "            lora_alpha=32, # Scaling factor that balances LoRA updates vs. base model\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],  # which model layers (e.g., q_proj, v_proj) to apply LoRA to.\n",
    "            lora_dropout=0.05, # Dropout rate for adapter layers.\n",
    "            bias=\"none\", # Whether to fine-tune bias terms.\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "\n",
    "        # In above,  LoRA applies to query and value projections of the attention layers — the most impactful and memory-heavy parts.\n",
    "\n",
    "        # Apply LoRA on quantization model. \n",
    "        self.model = get_peft_model(\n",
    "            self.model, \n",
    "            config\n",
    "        )\n",
    "\n",
    "        self.model.print_trainable_parameters()\n",
    "\n",
    "    def load_and_tokenize_dataset(self):\n",
    "        \"\"\"This function will load the data and it will performs the tokenization.\"\"\"\n",
    "        print(\"Load data and tokenization. \")\n",
    "        data = load_dataset(self.dataset_name, 'main', split=\"train\")\n",
    "        data_df = data.to_pandas()\n",
    "        \n",
    "        text_column = data_df.columns[0]\n",
    "        print(text_column)\n",
    "\n",
    "        if \"question\" in data_df.columns and \"answer\" in data_df.columns:\n",
    "            data_df[\"text\"] = data_df.apply(lambda x: f\"question: {x['question']} answer: {x['answer']}\", axis=1)\n",
    "            print(data_df['text'])\n",
    "        else:\n",
    "            data_df['text'] = data_df[text_column]\n",
    "        \n",
    "        # Convert back to hugging face dataset. \n",
    "        data = Dataset.from_pandas(data_df)\n",
    "\n",
    "        # Tokenizer dataset. \n",
    "        def tokenize(sample):\n",
    "            return self.tokenizer(sample['text'], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "        self.tokenized_data = data.map(\n",
    "            tokenize, \n",
    "            batched = True, \n",
    "            desc = \"Tokenizing data\", \n",
    "            remove_columns = data.column_names\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"This function will perform the training.\"\"\"\n",
    "        print(\"Running train. \")\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir = self.output_dir, \n",
    "            per_device_train_batch_size = batch_size, \n",
    "            gradient_accumulation_steps = 1, \n",
    "            learning_rate = learning_rate, \n",
    "            lr_scheduler_type = \"cosine\",\n",
    "            save_strategy = \"epoch\",\n",
    "            logging_steps = 100, \n",
    "            max_steps = max_steps, \n",
    "            num_train_epochs = epochs, \n",
    "            push_to_hub = True, \n",
    "            report_to = \"none\"\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model = self.model, \n",
    "            train_dataset = self.tokenized_data, \n",
    "            args = training_args, \n",
    "            data_collator = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "    def merge_and_save_model(self, model_repo):\n",
    "        \"\"\"This function is to merge and save the model.\"\"\"\n",
    "\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name, \n",
    "            trust_remote_code=True, \n",
    "            torch_dtype=torch.float32\n",
    "        )\n",
    "        peft_model = PeftModel.from_pretrained(\n",
    "            base_model, \n",
    "            self.output_dir, \n",
    "            from_transfomers = True\n",
    "        )\n",
    "        merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "        merged_model.push_to_hub(model_repo)\n",
    "        print(\"Merged and saving the model...!\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"This function exectue all the process. \"\"\"\n",
    "        print(\"starting fine-tunning process. \")\n",
    "\n",
    "        self.load_tokenizer()\n",
    "        print(\"Tokenizer Loaded.\")\n",
    "        \n",
    "        self.load_model()\n",
    "        print(\"Model Loaded\")\n",
    "\n",
    "        self.apply_lora()\n",
    "        print(\"Implementing LoRA.\")\n",
    "\n",
    "        self.load_and_tokenize_dataset()\n",
    "        print(\"Dataset loaded and tokenized.\")\n",
    "\n",
    "        self.train()\n",
    "        print(\"Train the model.\")\n",
    "\n",
    "        self.merge_and_save_model()\n",
    "        print(\"Merge and save the model.\")\n",
    "\n",
    "# \n",
    "model_name = \"microsoft/phi-1_5\"\n",
    "dataset_name = \"gsm8k\"\n",
    "output_dir = \"phi-1_5-finetuned\"\n",
    "\n",
    "fine_tunner = LoRAFineTuner(model_name,dataset_name,output_dir)\n",
    "\n",
    "fine_tunner.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
