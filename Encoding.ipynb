{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3106b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n--------------------------------------------------------------------------------\\n1. One Hot Encoding. \\n    Text                    O/P\\n    D1 The food is good      1\\n    D2 The food is bad       0\\n    D3 Pizza is Amazing      1\\n\\nTotal Vocabulary or Unique words: \\nThe food is good bad Pizza Amazing\\n1   0    0  0    0   0     0\\n0   1    0  0    0   0     0\\n0   0    1  0    0   0     0\\n0   0    0  1    0   0     0\\n0   0    0  0    1   0     0\\n0   0    0  0    0   1     0\\n0   0    0  0    0   0     1\\n\\nD1 [[1 0 0 0 0 0 0], [0 1 0 0 0 0 0], [0 0 1 0 0 0 0], [0 0 0 1 0 0 0]] Dim: 4x7\\nD2 [[1 0 0 0 0 0 0], [0 1 0 0 0 0 0], [0 0 1 0 0 0 0], [0 0 0 1 0 0 0]] Dim: 4x7\\nS3 [[0 0 0 0 0 1 0], [0 0 1 0 0 0 0], [0 0 0 0 0 0 1]] Dim: 3x7\\nAnd similar. \\n\\n# Advantages of One Hot Encoding. \\n-> Easy to implement with Python. \\n    Like OneHotEncoder, pd.get_dummies\\n\\n# Disadvantages of One Hot Encoding. \\n-> Creates sparse matrix meaning matrix containing lots of zeros which leads to overfitting. \\n-> ML Algorithms Expect fixed size inputs.\\n-> We cannot calculate the semantic meaning like static like \"river bank\" and \"bank money\".\\n-> Out of Vocabulary. When the words does not present in the dataset, In this case, Out of Vocabulary.\\n\\n--------------------------------------------------------------------------------\\n2. Bags of Words. \\nDataset. \\nText                    O/P\\nHe is a good boy         1\\nShe is a good girl       1\\nBoy and girl are good    1\\n\\nsteps:\\n    - Lower all the words. \\n    - apply stopwords. \\n    \\n\\n3. Tf-IDF\\n\\n4. Word2Vec. \\n\\n5. AverageWord2Vec.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "--------------------------------------------------------------------------------\n",
    "1. One Hot Encoding. \n",
    "    Text                    O/P\n",
    "    D1 The food is good      1\n",
    "    D2 The food is bad       0\n",
    "    D3 Pizza is Amazing      1\n",
    "\n",
    "Total Vocabulary or Unique words:\n",
    "The food is good bad Pizza Amazing\n",
    "1   0    0  0    0   0     0\n",
    "0   1    0  0    0   0     0\n",
    "0   0    1  0    0   0     0\n",
    "0   0    0  1    0   0     0\n",
    "0   0    0  0    1   0     0\n",
    "0   0    0  0    0   1     0\n",
    "0   0    0  0    0   0     1\n",
    "\n",
    "D1 [[1 0 0 0 0 0 0], [0 1 0 0 0 0 0], [0 0 1 0 0 0 0], [0 0 0 1 0 0 0]] Dim: 4x7\n",
    "D2 [[1 0 0 0 0 0 0], [0 1 0 0 0 0 0], [0 0 1 0 0 0 0], [0 0 0 1 0 0 0]] Dim: 4x7\n",
    "S3 [[0 0 0 0 0 1 0], [0 0 1 0 0 0 0], [0 0 0 0 0 0 1]] Dim: 3x7\n",
    "And similar. \n",
    "\n",
    "    # Advantages of One Hot Encoding. \n",
    "    -> Easy to implement with Python. \n",
    "        Like OneHotEncoder, pd.get_dummies\n",
    "\n",
    "    # Disadvantages of One Hot Encoding. \n",
    "    -> Creates sparse matrix meaning matrix containing lots of zeros which leads to overfitting. \n",
    "    -> ML Algorithms Expect fixed size inputs.\n",
    "    -> We cannot calculate the semantic meaning like static like \"river bank\" and \"bank money\".\n",
    "    -> Out of Vocabulary. When the words does not present in the dataset, In this case, Out of Vocabulary.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "2. Bags of Words. \n",
    "Dataset. \n",
    "Text                    O/P\n",
    "He is a good boy         1\n",
    "She is a good girl       1\n",
    "Boy and girl are good    1\n",
    "\n",
    "steps:\n",
    "    - Lower all the words. \n",
    "        S1: he is a good boy\n",
    "        S2: she is a good boy\n",
    "        S3: boy and girl are good\n",
    "    - Apply stopwords. \n",
    "        he, is, a, she, and, are -->  gets deleted. \n",
    "        S1: good boy\n",
    "        S2: good girl\n",
    "        S3: boy girl good\n",
    "\n",
    "    Total Unique Vocabulary             Frequency\n",
    "        good                                3\n",
    "        boy                                 2\n",
    "        girl                                2\n",
    "\n",
    "        then sort the frequency in the descending order which is already in order. \n",
    "        Based on the top most frequency, I will make at as a features. \n",
    "        Like this. \n",
    "            good        boy      girl         0/P\n",
    "        S1:  [1          1        0]           1\n",
    "        S2:  [0          0        1]           1\n",
    "        S3:  [1          1        1]           1\n",
    "            What if suppose 'good' increased or repeated, Just increased the count \n",
    "        Binary BoW: Even the word repeated, it forced to 1. [1 or 0]\n",
    "        But in case of the Normal BoW, word count gets updated based on the frequency.\n",
    "\n",
    "        Advantages:\n",
    "        - Easy to Implement and Intuitive. \n",
    "        - Fixed Sized I/P. \n",
    "\n",
    "        Dis-Advantages:\n",
    "        - Sparse Matrix on arrays. \n",
    "        - Ordering of the words is getting changed, means the meaning of the words gets changed. \n",
    "        - Out of Vocabulary(OOV) still issues here. \n",
    "        - Semantic Meaning is still not getting captured. \n",
    "\n",
    "3. Tf-IDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "Term Frequency (TF) = No. of repetition of words in sentence / No. of words in sentence\n",
    "Inverse Document Frequency = log(No. of sentences/No. of sentences containing the words)\n",
    "\n",
    "s1 --> good boy\n",
    "s2 --> good girl\n",
    "s3 --> boy girl good\n",
    "\n",
    "            Term Frequency\n",
    "            s1      s2      s3\n",
    "    good    1/2     1/2     1/3\n",
    "    boy     1/2     0/2     1/3\n",
    "    girl    0/2     1/2     1/3\n",
    "\n",
    "            Inverse Document Frequency\n",
    "    Words       IDF\n",
    "    good        log_e(3/3)\n",
    "    boy         log_e(3/2)\n",
    "    girl        log_e(3/2)\n",
    "\n",
    "                TF-IDF\n",
    "        good     boy                girl                    0/P\n",
    "    s1   0        1/2 * log_e(3/2)    0                      \n",
    "    s2   0        0                   1/2 * log_e(3/2)\n",
    "    s3   0        1/3 * log_e(3/2)    1/3 * log_e(3/2)\n",
    "\n",
    "    Advantages\n",
    "    - Intuitive. \n",
    "    - Fixed Size -> Vocab size. \n",
    "    - Word Importance is getting Captured. \n",
    "\n",
    "    Dis-Advantages\n",
    "    - sparsity still exist. \n",
    "    - OOV.\n",
    "\n",
    "4. Word2Vec. or What is Word Embedding.\n",
    "- representation of words.  so, that words are closer in the vector space. \n",
    "Eg: Happy  Excited\n",
    "\n",
    "- Word2Vec: Uses a Neural Network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector.\n",
    "\n",
    "5. AverageWord2Vec.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a774ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35719424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24f5b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc609009",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
