{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357dc3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Direct Preference Optimization (DPO) in LLMs:-\n",
    "- Offers a stable, performant, and computationally lightweight approach to fine-tuning language models.\n",
    "\n",
    "Note:- RLHF is complex and often unstable, requiring significant hyperparameter tuning and sampling from the language model.\n",
    "\n",
    "## What is Direct Preference Optimization (DPO) ?\n",
    "-> Direct Supervised Learning. \n",
    "-> Algorithm used: Simple loss function. \n",
    "\n",
    "## Reinforcement Learning (RL) ?\n",
    "-> Reward based learning. \n",
    "-> Algorithm used: PPO, Q-Learning, Actor-Critic. \n",
    "-> Removes the reward model fitting.\n",
    "\n",
    "# The Two Stages of DPO\n",
    "1. Supervised Fine-tuning (SFT):\n",
    "- Supervised fine-tuning ensures that the model’s responses align more closely with specific requirements and preferences.\n",
    "- For example, a conversational AI model could be finetuned to provide user-friendly and detailed responses that resonate with a company’s tone and branding.\n",
    "\n",
    "2. Preference Learning:\n",
    "- After the supervised fine-tuning stage, the model undergoes preference learning using preference data. \n",
    "- Preference data consists of curated options or alternatives related to a specific prompt. Annotators rank these options based on human preferences, providing insights into fine-tuning the model to produce outputs that align with human expectations.\n",
    "\n",
    "The simplicity of DPO lies in its direct definition of the preference loss as a function of the policy, eliminating the need for training a reward model first. During the fine-tuning phase, DPO treats the language model itself as the reward model. It optimizes the policy using a binary cross-entropy objective, leveraging human preference data to determine preferred responses. By comparing the model’s responses to the preferred ones, the policy is adjusted to enhance its performance.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8238db28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndpo_trainer.train()\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Supervised Fine-Tuning (SFT):\n",
    "# Begin by training the SFT model using a dataset that is in-distribution and relevant to the task at hand. This step sets the stage for the DPO algorithm to work effectively.\n",
    "\n",
    "'''\n",
    "from transformers import AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "dataset = load_dataset(\"your-domain-dataset\", split=\"train\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"your-foundation-model-of-choice\")\n",
    "trainer = SFTTrainer(model, train_dataset=dataset, dataset_text_field=\"text\", max_seq_length=512)\n",
    "trainer.train()\n",
    "'''\n",
    "\n",
    "# Understanding the Dataset Format:\n",
    "# The DPO trainer requires a specific dataset format that reflects the preference between two sentences. The dataset should include entries for the prompt, chosen responses, and rejected responses.\n",
    "\n",
    "'''\n",
    "dpo_dataset_dict = {\n",
    "    \"prompt\": [\"hello\", \"how are you\", ...],\n",
    "    \"chosen\": [\"hi, nice to meet you\", \"I am fine\", ...],\n",
    "    \"rejected\": [\"leave me alone\", \"I am not fine\", ...],\n",
    "}\n",
    "'''\n",
    "\n",
    "# Leveraging the DPOTrainer:\n",
    "# Initialize the DPOTrainer, specifying the model to be trained, a reference model (used to compute implicit rewards), the beta hyperparameter for the implicit reward, and the dataset.\n",
    "'''\n",
    "dpo_trainer = DPOTrainer(\n",
    "        model,\n",
    "        model_ref,\n",
    "        args=training_args,\n",
    "        beta=0.1,\n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    ")\n",
    "'''\n",
    "\n",
    "# Training and Monitoring:\n",
    "# Begin the training process with dpo_trainer.train() Monitor the model’s performance using reward metrics such as rewards/chosen, rewards/rejected, rewards/accuracies, and rewards/margins.\n",
    "'''\n",
    "dpo_trainer.train()\n",
    "'''\n",
    "\n",
    "# Advantages of DPO over RLHF\n",
    "\n",
    "# Direct Preference Optimization (DPO) offers several advantages over Reinforcement Learning from Human Feedback (RLHF). DPO is stable, performant, and computationally lightweight, eliminating the need for reward model fitting and extensive hyperparameter tuning. Additionally, DPO has shown promising results in controlling sentiment, improving response quality in summarization and single-turn dialogue, and simplifying the implementation and training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "031ca1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./venv/lib/python3.13/site-packages (4.52.4)\n",
      "Collecting trl\n",
      "  Using cached trl-0.18.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: peft in ./venv/lib/python3.13/site-packages (0.15.2)\n",
      "Requirement already satisfied: accelerate in ./venv/lib/python3.13/site-packages (1.8.0)\n",
      "Requirement already satisfied: datasets in ./venv/lib/python3.13/site-packages (3.6.0)\n",
      "Requirement already satisfied: bitsandbytes in ./venv/lib/python3.13/site-packages (0.42.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./venv/lib/python3.13/site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.13/site-packages (from transformers) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.13/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./venv/lib/python3.13/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./venv/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.4)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.13/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in ./venv/lib/python3.13/site-packages (from peft) (2.7.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./venv/lib/python3.13/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./venv/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.13/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: xxhash in ./venv/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./venv/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./venv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.13/site-packages (from bitsandbytes) (1.15.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in ./venv/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.13/site-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.13/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading trl-0.18.2-py3-none-any.whl (366 kB)\n",
      "Installing collected packages: trl\n",
      "Successfully installed trl-0.18.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers trl peft accelerate datasets bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e9031a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "75abae3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple GPU via MPS\n"
     ]
    }
   ],
   "source": [
    "# Check for MPS (Metal Performance Shaders) GPU support\n",
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple GPU via MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2821ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTTrainer, DPOConfig, DPOTrainer\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_config\n",
    "import torch\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2108e5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in ./venv/lib/python3.13/site-packages (0.42.0)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.13/site-packages (from bitsandbytes) (1.15.3)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in ./venv/lib/python3.13/site-packages (from scipy->bitsandbytes) (2.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cc6773b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameter for the SFT\n",
    "sft_config={\n",
    "            \"model_ckpt\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", #This is my Pretrain model\n",
    "            \"load_in_4bit\": True,\n",
    "            \"device_map\": {\"\": Accelerator().local_process_index},\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"trust_remote_code\": True,\n",
    "\n",
    "            \"use_lora\": True,\n",
    "            \"r\": 16,\n",
    "            \"lora_alpha\": 16,\n",
    "            \"lora_dropout\": 0.05,\n",
    "            \"bias\": \"none\",\n",
    "            \"task_type\": \"CAUSAL_LM\",\n",
    "            \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
    "\n",
    "            \"output_dir\": \"sft-tiny-chatbot\",\n",
    "            \"per_device_train_batch_size\": 1,\n",
    "            \"gradient_accumulation_steps\": 1,\n",
    "            \"optim\": \"paged_adamw_32bit\",\n",
    "            \"learning_rate\": 2e-4,\n",
    "            \"lr_scheduler_type\": \"cosine\",\n",
    "            \"save_strategy\": \"epoch\",\n",
    "            \"logging_steps\": 100,\n",
    "            \"num_train_epochs\": 1,\n",
    "            \"max_steps\": 250,\n",
    "            \"fp16\": True,\n",
    "            \"push_to_hub\": True,\n",
    "            \"packing\": False,\n",
    "            \"max_seq_length\": 512,\n",
    "            \"neftune_noise_alpha\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d610aeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainSFT:\n",
    "    def __init__(self, data, config) -> None:\n",
    "        self.data = data\n",
    "        self.config = config\n",
    "\n",
    "    def prepare_lora_model(self):\n",
    "        self.lora_config = LoraConfig(\n",
    "            r = self.config[\"r\"],\n",
    "            lora_alpha = self.config[\"lora_alpha\"],\n",
    "            lora_dropout = self.config[\"lora_dropout\"],\n",
    "            bias = self.config[\"bias\"],\n",
    "            task_type = self.config[\"task_type\"],\n",
    "            target_modules = self.config[\"target_modules\"]\n",
    "        )\n",
    "\n",
    "        self.model = get_peft_config(\n",
    "            self.model,\n",
    "            self.lora_config\n",
    "        )\n",
    "        \n",
    "    def load_model_tokenizer(self):\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.config[\"model_ckpt\"],\n",
    "            load_in_4bit = self.config[\"load_in_4bit\"],\n",
    "            device_map = self.config[\"device_map\"],\n",
    "            torch_dtype = self.config[\"torch_dtype\"]\n",
    "        )\n",
    "\n",
    "        self.model.config.use_cache = False\n",
    "        self.model.config.pretraining_tp = 1\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "\n",
    "    def set_training_arguments(self):\n",
    "        return TrainingArguments(\n",
    "            output_dir = self.config[\"output_dir\"],\n",
    "            per_device_train_batch_size = self.config[\"per_device_train_batch_size\"],\n",
    "            gradient_accumulation_steps = self.config[\"gradient_accumulation_steps\"],\n",
    "            optim = self.config[\"optim\"],\n",
    "            learning_rate = self.config[\"learning_rate\"],\n",
    "            lr_scheduler_type = self.config[\"lr_scheduler_type\"],\n",
    "            save_strategy = self.config[\"save_strategy\"],\n",
    "            logging_steps = self.config[\"logging_steps\"],\n",
    "            num_train_epochs = self.config[\"num_train_epochs\"],\n",
    "            max_steps = self.config[\"max_steps\"],\n",
    "            fp16 = self.config[\"fp16\"],\n",
    "            push_to_hub = self.config[\"push_to_hub\"],\n",
    "            neftune_noise_alpha = self.config[\"neftune_noise_alpha\"],\n",
    "            report_to = \"none\"\n",
    "        )\n",
    "\n",
    "    def create_trainer(self):\n",
    "        self.load_model_tokenizer()\n",
    "\n",
    "        if self.config[\"use_lora\"]:\n",
    "            print(self.model.print_trainable_parameter())\n",
    "            self.trainer = SFTTrainer(\n",
    "                model = self.model,\n",
    "                train_dataset = self.data, \n",
    "                perf_config = self.lora_config, \n",
    "                args = self.set_training_args(), \n",
    "                tokenizer = self.tokenizer\n",
    "            )\n",
    "                \n",
    "    def train_and_save_model(self):\n",
    "        self.create_trainer()\n",
    "        self.trainer.train()\n",
    "        self.trainer.save_model(self.config[\"output_dir\"])\n",
    "        self.tokenizer.save_pretrained(self.config[\"output_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8f0bac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data():\n",
    "    data = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
    "    data_df = data.to_pandas()\n",
    "    data_df = data_df[:700]\n",
    "    data_df['text'] = data_df[[\"input\", \"instruction\", \"output\"]].apply(lambda x: \"Human\" + x[\"instruction\"] + \" \" + x[\"input\"] + \" Assistant: \" + x[\"output\"], axis=1)\n",
    "    data = Dataset.from_pandas(data_df)\n",
    "    return data\n",
    "\n",
    "data = create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9077d700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'text'],\n",
       "    num_rows: 700\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1bea6095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Give three tips for staying healthy.',\n",
       " 'input': '',\n",
       " 'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.',\n",
       " 'text': 'HumanGive three tips for staying healthy.  Assistant: 1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "74650781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m train_sft = TrainSFT(data, sft_config)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrain_sft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_and_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mTrainSFT.train_and_save_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_and_save_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.train()\n\u001b[32m     67\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.save_model(\u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33moutput_dir\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mTrainSFT.create_trainer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_trainer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_model_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33muse_lora\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     55\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m.model.print_trainable_parameter())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mTrainSFT.load_model_tokenizer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_model_tokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_ckpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mload_in_4bit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdevice_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtorch_dtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.config.use_cache = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.config.pretraining_tp = \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/genai_bootcamp/venv/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/genai_bootcamp/venv/lib/python3.13/site-packages/transformers/modeling_utils.py:309\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    311\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/genai_bootcamp/venv/lib/python3.13/site-packages/transformers/modeling_utils.py:4390\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4387\u001b[39m     hf_quantizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4390\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4394\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4396\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4397\u001b[39m     torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)\n\u001b[32m   4398\u001b[39m     device_map = hf_quantizer.update_device_map(device_map)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/genai_bootcamp/venv/lib/python3.13/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:76\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     73\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing `bitsandbytes` 4-bit quantization requires Accelerate: `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m     )\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_available():\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     77\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     78\u001b[39m     )\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001b[31mImportError\u001b[39m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "source": [
    "train_sft = TrainSFT(data, sft_config)\n",
    "train_sft.train_and_save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2851c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca4c81e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61a0e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37fa460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09cc48f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "657f1ab3",
   "metadata": {},
   "source": [
    "#### **Preference Alignment - DPO**\n",
    "\n",
    "Will train or finetune our model using PEFT(SFT) then will retrain for preference alignment for controlled response using DPO. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8bb2e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_config = {\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e13688d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDPO:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def prepare_lora_model(self):\n",
    "        pass\n",
    "\n",
    "    def load_model_tokenizer(self):\n",
    "        pass\n",
    "\n",
    "    def set_training_args(self):\n",
    "        return DPOConfig()\n",
    "    \n",
    "    def create_trainer(self):\n",
    "        pass\n",
    "\n",
    "    def train_and_save_model(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad52e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data():\n",
    "    return data\n",
    "\n",
    "data = create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc09813",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_dpo = TrainDPO(\u001b[43mdata\u001b[49m, dpo_config)\n",
      "\u001b[31mNameError\u001b[39m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "train_dpo = TrainDPO(data, dpo_config)\n",
    "train_dpo.train_and_save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea394e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ab3768c",
   "metadata": {},
   "source": [
    "1. Unsupervised pretraining\n",
    "2. SFT\n",
    "3. DPO\n",
    "4. Inferencing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
