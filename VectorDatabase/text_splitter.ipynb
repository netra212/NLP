{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f30a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Data -> pdf -> 100 pages -> text, tables, Images (embeddings) -> vectorDB\n",
    "\n",
    "# text ---My name is Netra [0.1, 0.2, 0.5, 0.4] --> VectorDBs --> Document + Embedding is store in VectorDB. \n",
    "\n",
    "# Document contains:\n",
    "    - metadata\n",
    "    - page_content\n",
    "    - embedding\n",
    "\n",
    "# When to perform the chunking. \n",
    "    1. If the capacity of the model is small. \n",
    "    2. Fast Retrieval. \n",
    "    3. Computational Resources. \n",
    "    4. If need to improve the representation quality of the data. \n",
    "\n",
    "# ‚úÖ What is Chunking?\n",
    "- Chunking = Splitting large documents (text, PDFs, webpages) into smaller parts or ‚Äúchunks‚Äù before generating embeddings.\n",
    "\n",
    "- For example:\n",
    "    a. A 100-page PDF ‚ûù split into 500 small chunks (paragraphs, sentences, etc.)\n",
    "    b. Each chunk ‚ûù converted to vector (embedding) ‚ûù stored in a vector database.\n",
    "\n",
    "# üîç Why and When to Perform Chunking ?\n",
    "    1. üìè Model Context Limitations\n",
    "        - üß† Chunking ensures that each piece fits into the model's max token size.\n",
    "\n",
    "    2. üöÄ Faster and More Accurate Retrieval\n",
    "        - ‚ÄúFast Retrieval‚Äù\n",
    "\n",
    "        - Smaller chunks = finer-grained search.\n",
    "        - When a user asks a question, the retriever can fetch only the most relevant chunk, not an entire page or document.\n",
    "        - This increases accuracy and relevance of responses from the LLM.\n",
    "        - üìå Example: If you store full pages or full chapters, your retrieval may return too much irrelevant data.\n",
    "\n",
    "    3. üßÆ Optimizing Computational Resources\n",
    "        - ‚ÄúComputational Resources‚Äù\n",
    "\n",
    "        - Embedding large chunks uses more memory and time.\n",
    "        - Embedding smaller chunks is faster and easier to batch-process.\n",
    "        - Vector search is also faster when each entry is smaller and semantically tight.\n",
    "    \n",
    "    4. üéØ Improving Semantic Representation\n",
    "        - ‚ÄúImprove the representation quality‚Äù\n",
    "\n",
    "        - Large chunks often contain mixed topics, reducing embedding precision.\n",
    "        - Chunking helps isolate semantically consistent units, improving retrieval performance.\n",
    "        - Better representation = better search results.\n",
    "\n",
    "# Why do we need the Chunking ?\n",
    "    1. Model Limitation. \n",
    "    2. Handle non-uniform document lenght. \n",
    "    3. Improve representation of data. \n",
    "    4. Cost Retrieval since chunk we can regulate. \n",
    "    5. Optimize the computational resources. \n",
    "\n",
    "# chunk_size is a hyperparameter so we can't decide, that depends on the data and the models. \n",
    "\n",
    "# APPROACHES:\n",
    "    1. Length based approach.  \n",
    "        - Count length, tokens. \n",
    "        - token(word) based, character(individual) based. \n",
    "        \n",
    "    2. Text Structured based approach. \n",
    "        - Paragraph, sentence. \n",
    "\n",
    "    3. Document Structured based approach. \n",
    "        - MARKDOWN, HTML\n",
    "        - CODE (PROGRAMMING LANGUAGES)\n",
    "        - JSON\n",
    "\n",
    "    4. Semantic based approach.\n",
    "        - \n",
    "# \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552416d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b92d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406333e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8307f0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d014d903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab3651e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a3a25f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cad6599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
