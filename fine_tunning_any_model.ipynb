{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d687efb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in ./venv/lib/python3.13/site-packages (1.8.0)\n",
      "Requirement already satisfied: transformers in ./venv/lib/python3.13/site-packages (4.52.4)\n",
      "Requirement already satisfied: peft in ./venv/lib/python3.13/site-packages (0.15.2)\n",
      "Requirement already satisfied: bitsandbytes in ./venv/lib/python3.13/site-packages (0.42.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./venv/lib/python3.13/site-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.13/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.13/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in ./venv/lib/python3.13/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./venv/lib/python3.13/site-packages (from accelerate) (2.7.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in ./venv/lib/python3.13/site-packages (from accelerate) (0.33.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./venv/lib/python3.13/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.13/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./venv/lib/python3.13/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./venv/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.4)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.13/site-packages (from bitsandbytes) (1.15.3)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.13/site-packages (from requests->transformers) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate transformers peft bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb1e717c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./venv/lib/python3.13/site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.13/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./venv/lib/python3.13/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./venv/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.13/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./venv/lib/python3.13/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./venv/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./venv/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./venv/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in ./venv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./venv/lib/python3.13/site-packages (from datasets) (0.33.0)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.13/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.13/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./venv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in ./venv/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./venv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23415146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Desktop/genai_bootcamp/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig, DataCollatorForLanguageModeling)\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ba790b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in ./venv/lib/python3.13/site-packages (0.42.0)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.13/site-packages (from bitsandbytes) (1.15.3)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in ./venv/lib/python3.13/site-packages (from scipy->bitsandbytes) (2.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "520ea8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./venv/lib/python3.13/site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in ./venv/lib/python3.13/site-packages (0.22.1)\n",
      "Requirement already satisfied: torchaudio in ./venv/lib/python3.13/site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.13/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.13/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.13/site-packages (from torchvision) (2.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.13/site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc11343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple GPU via MPS\n"
     ]
    }
   ],
   "source": [
    "# Check for MPS (Metal Performance Shaders) GPU support\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple GPU via MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd4e760a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!touch .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42b00224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e844ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628d25e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization of the class. \n",
      "starting fine-tunning process. \n",
      "Loaded Tokenizer.\n",
      "Tokenizer Loaded.\n",
      "Loaded Model.\n",
      "Model Loaded\n",
      "Implementing Lora. \n",
      "trainable params: 3,145,728 || all params: 1,421,416,448 || trainable%: 0.2213\n",
      "Implementing LoRA.\n",
      "Load data and tokenization. \n",
      "question\n",
      "0       question: Natalia sold clips to 48 of her frie...\n",
      "1       question: Weng earns $12 an hour for babysitti...\n",
      "2       question: Betty is saving money for a new wall...\n",
      "3       question: Julie is reading a 120-page book. Ye...\n",
      "4       question: James writes a 3-page letter to 2 di...\n",
      "                              ...                        \n",
      "7468    question: Very early this morning, Elise left ...\n",
      "7469    question: Josh is saving up for a box of cooki...\n",
      "7470    question: Colin can skip at six times the spee...\n",
      "7471    question: Janet, a third grade teacher, is pic...\n",
      "7472    question: At 30, Anika is 4/3 the age of Maddi...\n",
      "Name: text, Length: 7473, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing data: 100%|██████████| 7473/7473 [00:00<00:00, 7759.96 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded and tokenized.\n",
      "Running train. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Desktop/genai_bootcamp/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:49, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.139600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the model.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LoRAFineTuner.merge_and_save_model() missing 1 required positional argument: 'model_repo'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 168\u001b[39m\n\u001b[32m    164\u001b[39m output_dir = \u001b[33m\"\u001b[39m\u001b[33mphi-1_5-finetuned\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    166\u001b[39m fine_tunner = LoRAFineTuner(model_name,dataset_name,output_dir)\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m \u001b[43mfine_tunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 158\u001b[39m, in \u001b[36mLoRAFineTuner.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28mself\u001b[39m.train()\n\u001b[32m    156\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTrain the model.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmerge_and_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMerge and save the model.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: LoRAFineTuner.merge_and_save_model() missing 1 required positional argument: 'model_repo'"
     ]
    }
   ],
   "source": [
    "class LoRAFineTuner:\n",
    "    def __init__(self, model_name,dataset_name,output_dir):\n",
    "        \"\"\"Initialization of the class parameter. \n",
    "        \"\"\"\n",
    "        print(\"Initialization of the class. \")\n",
    "        self.model_name = model_name\n",
    "        self.dataset_name = dataset_name\n",
    "        self.output_dir = output_dir\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.tokenized_data = None\n",
    "\n",
    "    def load_tokenizer(self):\n",
    "        \"\"\"This function to define the tokenizer of the model.\"\"\"\n",
    "        print(\"Loaded Tokenizer.\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"This function to define the model.\"\"\"\n",
    "        print(\"Loaded Model.\")\n",
    "\n",
    "        # Bits and Bytes Configs for the Quantized Models. \n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit = True, # Load the model weights in 4-bit precision (instead of 16/32-bit)\n",
    "            bnb_4bit_use_double_quant = True, # Enabling the double quantization which helps in preserving the accuracy. \n",
    "            bnb_4bit_quant_type = \"nf4\", # normal float 4, a quantized type optimized for transformers.  \n",
    "            bnb_4bit_compute_dtype = torch.float16 # Reducing the precisions of the model. \n",
    "        )\n",
    "        \n",
    "        # Quantization model.\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name, \n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code = True # allowing custom model to use. \n",
    "            # quantization_config = bnb_config # Applies the 4-bit quantization setup from earlier\n",
    "        )\n",
    "\n",
    "        self.model.config.use_cache = False # During training, using cache = True, may causes issues with backpropagation in casual langauge model. Disabling it ensures the model doesn’t store outputs for reuse (which is good for inference, but bad for training).\n",
    "\n",
    "    def apply_lora(self):\n",
    "        \"\"\"this function to define the lora model of the model. \"\"\"\n",
    "        print(\"Implementing Lora. \")\n",
    "        # LoRA - Low rank adapter to save compute and memory while training.\n",
    "        config = LoraConfig(\n",
    "            r=16,  # size of the low-rank matrices, smaller is better to save the memory. \n",
    "            lora_alpha=32, # Scaling factor that balances LoRA updates vs. base model\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],  # which model layers (e.g., q_proj, v_proj) to apply LoRA to.\n",
    "            lora_dropout=0.05, # Dropout rate for adapter layers.\n",
    "            bias=\"none\", # Whether to fine-tune bias terms.\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "\n",
    "        # In above,  LoRA applies to query and value projections of the attention layers — the most impactful and memory-heavy parts.\n",
    "\n",
    "        # Apply LoRA on quantization model. \n",
    "        self.model = get_peft_model(\n",
    "            self.model, \n",
    "            config\n",
    "        )\n",
    "\n",
    "        self.model.print_trainable_parameters()\n",
    "\n",
    "    def load_and_tokenize_dataset(self):\n",
    "        \"\"\"This function will load the data and it will performs the tokenization.\"\"\"\n",
    "        print(\"Load data and tokenization. \")\n",
    "        data = load_dataset(self.dataset_name, 'main', split=\"train\")\n",
    "        data_df = data.to_pandas()\n",
    "        \n",
    "        text_column = data_df.columns[0]\n",
    "        print(text_column)\n",
    "\n",
    "        if \"question\" in data_df.columns and \"answer\" in data_df.columns:\n",
    "            data_df[\"text\"] = data_df.apply(lambda x: f\"question: {x['question']} answer: {x['answer']}\", axis=1)\n",
    "            print(data_df['text'])\n",
    "        else:\n",
    "            data_df['text'] = data_df[text_column]\n",
    "        \n",
    "        # Convert back to hugging face dataset. \n",
    "        data = Dataset.from_pandas(data_df)\n",
    "\n",
    "        # Tokenizer dataset. \n",
    "        def tokenize(sample):\n",
    "            return self.tokenizer(sample['text'], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "        self.tokenized_data = data.map(\n",
    "            tokenize, \n",
    "            batched = True, \n",
    "            desc = \"Tokenizing data\", \n",
    "            remove_columns = data.column_names\n",
    "        )\n",
    "\n",
    "    def train(self, epochs: int = 1, batch_size: int = 4, learning_rate: float = 2e-4, max_steps: int = 100):\n",
    "        \"\"\"This function will perform the training.\"\"\"\n",
    "        print(\"Running train. \")\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir = self.output_dir, \n",
    "            per_device_train_batch_size = batch_size, \n",
    "            gradient_accumulation_steps = 1, \n",
    "            learning_rate = learning_rate, \n",
    "            lr_scheduler_type = \"cosine\",\n",
    "            save_strategy = \"epoch\",\n",
    "            logging_steps = 100, \n",
    "            max_steps = max_steps, \n",
    "            num_train_epochs = epochs, \n",
    "            push_to_hub = True, \n",
    "            report_to = \"none\"\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model = self.model, \n",
    "            train_dataset = self.tokenized_data, \n",
    "            args = training_args, \n",
    "            data_collator = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "    def merge_and_save_model(self, model_repo: str):\n",
    "        \"\"\"This function is to merge and save the model.\"\"\"\n",
    "\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name, \n",
    "            trust_remote_code=True, \n",
    "            torch_dtype=torch.float32\n",
    "        )\n",
    "        peft_model = PeftModel.from_pretrained(\n",
    "            base_model, \n",
    "            self.output_dir, \n",
    "            from_transfomers = True\n",
    "        )\n",
    "        merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "        merged_model.push_to_hub(model_repo)\n",
    "        print(\"Merged and saving the model...!\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"This function exectue all the process. \"\"\"\n",
    "        print(\"starting fine-tunning process. \")\n",
    "\n",
    "        self.load_tokenizer()\n",
    "        print(\"Tokenizer Loaded.\")\n",
    "        \n",
    "        self.load_model()\n",
    "        print(\"Model Loaded\")\n",
    "\n",
    "        self.apply_lora()\n",
    "        print(\"Implementing LoRA.\")\n",
    "\n",
    "        self.load_and_tokenize_dataset()\n",
    "        print(\"Dataset loaded and tokenized.\")\n",
    "\n",
    "        self.train()\n",
    "        print(\"Train the model.\")\n",
    "\n",
    "        self.merge_and_save_model()\n",
    "        print(\"Merge and save the model.\")\n",
    "\n",
    "# \n",
    "model_name = \"microsoft/phi-1_5\"\n",
    "dataset_name = \"gsm8k\"\n",
    "output_dir = \"phi-1_5-finetuned\"\n",
    "\n",
    "fine_tunner = LoRAFineTuner(model_name,dataset_name,output_dir)\n",
    "\n",
    "fine_tunner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd8ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
